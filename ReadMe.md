Курсовая работа: Парсинг сайта гос закупки (Дьякович 21704)

Главная проблема и головная боль этого парсинга - это недоработка со стороны редакторов сайта
Одна и та же информация может находиться в разных объявлениях и в разных местах, это приходилось постоянно держать в
голове и не радоваться, когда закончен какой-либо метод, потому что на очередном объявлении все может сломаться

Также программа писалась долго и самые ранне-реализованные методы могут выглядеть не очень из-за отсутствия
каких-то знаний на момент написания, если скажете, перепишу


Остается нереализованным:
1. Отправка данных в базу (проблема, что не смог найти как адекватно сделать какой-то текст гиперссылкой на сайт или папку)
2. представление базы (может используя flask?, или подскажите, пожалуйста, с помощью чего представляете базу вы, я построюсь)
3. запуск кода по таймеру

Объяснение работы основоного кода программы изложено в самом main файле

Реализовано:
1. Получение данных с сайта:
    класс Parser, объяснение работы методов:
    1. Get_all_site_information - отправляет get запрос на сервер, создает экземпляр класса beautifulsoup и
        возвращает Resultset
    2. GetNextUrl - из него мы получаем url каждой следующей карточки закупки 
    3. relevance_checking - каждую новую карточку закупки проверяем, не достигли ли мы последней нужной
    4. look_at_prev_end_url - получаем наш последний url, если файла нет, он создастся (пытался оптимизировать,
       не получилось, остался небольшой костылек в виде 2-ух видов открытия файла в 1-ом методе)
    5. статический метод next_end_url_update - обновление конечного url в файле при его достижении
2. Обработка данных:
    класс DBwork, методы:
    1. get_page_information - подконтрольные ему методы: get_notice_id, get_purchase_and_company_name, get_date,
        get_price, get_stage и метод реализует соответственно получение id закупки, наименование закупки, название
        компании-закупщика, дату закупки, цену и стадию закупки (Вопрос - вы говорили, что нужно определять еще и
        в чьей собственности находится компания, государственная она или частная, но ведь у нас сайт называется
        гос закупки, какая еще может быть собственность?)
    2. look_at_curdare - получение даты заказа, нужна если мы достигли 100-ой страницы
    3. get_company_region - вынес его из get_page_information, потому что информация о регионе находится в другом месте
        для определения региона из текста страницы используются ключевые слова - названия кра, обл и тд, а также,
        если закупка происходила в городах Москва и Санкт-Петербург, то на сайте указывается город, а не регион, эти
        ситуации обрабатываются отдельно
   4. Documents_checking - проверяет, есть ли у закупки документы, и если есть создает нужные папки и возвращает url,
        по которому эти документы можно скачать
   5. Documents - непосредственно скачивает документы и кидает в отдельную папку
